\chapter{Neural Networks}
\label{neural_networks}

In this part we describe how to define $\mathcal{H}$ using functions called \textbf{neural networks}. One advantage of these functions is that they're easy to adapt to multi-task learning. We begin by describing how to design $\mathcal{H}$ using neural networks. We then turn to the issue of how to use $\mathcal{D}$ to search this hypothesis space. We then cover regularization techniques, the purpose of which is to prevent overfitting. Lastly, we introduce convolutional neural networks which are specialized functions often used for text classification problems such as relation extraction.

\input{feedForwardNeuralNetworks}
\input{learningAlgorithm}
\input{convolutionalNeuralNetworks}

\section{Summary}
In this section we have seen how to define $\mathcal{H}$ with neural networks, and we have seen how to search this space using backpropagation and gradient descent. Moreover, we have discussed regularization techniques that restrict the learning algorithm to a limited region of $\mathcal{H}$ in order to reduce the risk of overfitting. Finally, we have introduced convolutional neural networks that take advantage of convolutions as feature detectors.