\chapter{Multi-Task Learning}
\label{multi-task_learning}
In this section we introduce an extension to the supervised machine learning framework called multi-task learning. We first cover the main ideas and motivation for multi-task learning. We then summarize different variations on statistical learning theory to gain an intuition of how and when multi-task learning works. Finally, we describe how to implement multi-task learning with neural networks.

\input{mtlVersusStl}
\input{learningToLearn}
\input{representationLearning}
\input{taskRelatedness}
\input{deepMultiTaskLearning}

\section{Summary}
In this section we have introduced important contributions to statistical learning theory that shed light on some of the possible benefits of multi-task learning. Specifically, we have seen how multi-task learning can be seen as a form of bias learning that automatically learns a hypothesis space from a family of hypothesis spaces $\mathbb{H}$. Vapnik-Chervonkis style analysis of this view of multi-task learning shows that the distance between generalization and training error depends inversely on the number of tasks $M$. This means that multi-task learning is a feasible approach of re-using annotated data.
\\\\
We have discussed representation learning as a specific instance of bias learning. We have cited a result that shows that representation learning leads to much the same sample complexity dynamics as bias learning.
\\\\
Moreover, we have discussed the impact of learning related vs. unrelated tasks. We have seen that Vapnik-Chervonenkis analysis confirms our intuition that learning related tasks can provide stronger guarantees on the potential benefits for generalization error. We have also discussed efforts towards finding characteristics of target and auxiliary tasks that are good predictors of gains in generalization performance for multi-task learning. We have argued that despite these efforts, the most reliable approach for determining if multi-task learning is appropriate for a particular problem is through trial and error.
\\\\
Finally, we have discussed how to adapt neural networks for multi-task learning using hard weight sharing. We have presented a Vapnik-Chervonkenkis style analysis of deep multi-task learning that shows that the difficulty of learning a good representations for a set of tasks using hard neural network weight sharing is governed by the amount of target and auxiliary data and the the number of weights in the shared layers.
\\\\
In the next section we explain how we have applied this understanding of deep multi-task learning to implement an experiment that tests it's effectiveness in the context of relation classification.