\chapter{Multi-Task Learning}
\label{multi-task_learning}
In this section we introduce an extension to the supervised machine learning framework called multi-task learning. We first cover the main ideas and motivation for multi-task learning. We then summarize different variations on statistical learning theory to gain an intuition of how and when multi-task learning works. Finally, we describe how to implement multi-task learning with neural networks.

\input{mtlVersusStl}
\input{learningToLearn}
\input{representationLearning}
\input{taskRelatedness}
\input{deepMultiTaskLearning}

\section{Summary}
In this section we have introduced important contributions to statistical learning theory that shed light on the some of the possible benefits of multi-task learning. Specifically, we have seen how multi-task learning can be seen as a form of bias learning that automatically learns a hypothesis space. We have discussed representation learning as a specific instance of bias learning. Moreover, we have discussed the impact of learning related vs. unrelated tasks. Finally, we have discussed how to adapt neural networks for multi-task learning using hard weight sharing. We have presented a Vapnik-Chervonkenkis style analysis of deep multi-task learning that shows that the difficulty of learning a good representations for a set of tasks using hard neural network weight sharing is governed by the amount of target and auxiliary data and the the number of weights in the shared layers.