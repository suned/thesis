\section{Feed-Forward Neural Networks}
A feed-forward neural network is a function $h: \mathcal{X} \mapsto \mathcal{Y}$. To understand how it works, it's instructive to look at each part of its name in isolation.
\\\\
$h$ is called a \textbf{network} because it's a composition of $L$ \textbf{layers} of other functions $f^{(l)}$. Each $f^{(l)}$ receives input from $f^{(l-1)}$. For example if $L = 2$, then $h(\mathbf{x}) = f^{(2)}(f^{(1)}(\mathbf{x}))$. We denote the input to $f^{(1)}$ as $\mathbf{x}^{(0)}$, which is identical to the input vector $\mathbf{x}$, except for an added \textbf{bias} component of 1, as described later in this section. each $f^{(l)}$ outputs a vector $\mathbf{x}^{(l)}$ of dimension $d^{(l)}$ which is the input of $f^{(l+1)}$. The dimensionality of these vectors determine the \textbf{width} of the network. The number of layers $L$ is called the \textbf{depth} of the network. $f^{(L)}$ is called the \textbf{output layer}. The remaining functions $f^{(1)}$ to $f^{(L-1)}$ are called \textbf{hidden layers}. 
\\\\
The functions $f^{(1)}$ to $f^{(L)}$ are ordered by their index $l$. By ordered, we mean that the index of the innermost functions are smaller than the index of the outermost. $h$ is called a \textbf{feed-forward} network because each $f^{(l)}$ can receive input only from functions $f^{(i)}$ if $l > i$. In other words, it's not possible for a function $f^{(l)}$ to feed its own output into itself, or any other function that it receives input from.
\\\\
Finally $h$ is called a \textbf{neural} network since its design is loosely based on neurons in the brain \citep{goodfellow16}. Each component $x_i$ of the vector $\mathbf{x}^{(l)}$ can be seen as the output of a unit similar to a neuron. Each unit in layer $l$ receives input from units in layer $l-1$. The output $x^{(l-1)}_i$ of unit $i$ in layer $l-1$ is multiplied by a weight $w^{(l)}_{ij}$ that gives the strength of the connection between unit $i$ in $l-1$ and unit $j$ in $l$. Unit $j$ sums all of the input it receives from units in layer $l-1$ to obtain its \textbf{activation} $a^{(l)}_j = \sum_{i=0}^{d^{(l-1)}} w^{(l)}_{ij}x^{(l-1)}_{i}$. To compute its output $x^{(l)}_j$, it applies an \textbf{activation function} $\sigma(a^{(l)}_j)$ to its activation.

Activation functions model the behaviour of biological neurons by outputting a signal only when the activation is above a certain threshold. To make it possible to learn this threshold for each unit using the same activation function, we introduce a special \textbf{bias} unit that always outputs 1. The index of the bias unit in layer $l$ is 0 by convention. Figure \ref{connection}. shows how a unit $j$ computes its output $x^{(l)}_j$ by combining the outputs of units in layer $l-1$.

\tikzstyle{neuron}=[circle,draw,minimum size=20pt,inner sep=0pt]
\tikzstyle{summation} = [square, minimum size=20pt,inner sep=0pt]
\tikzstyle{edge} = [draw,thick,-]
\tikzstyle{weight} = [font=\small]

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->, >=stealth, swap]
			\node [neuron] (sigma1) at (0,0) {$i$};
			\node [neuron] (sigma2) at (6,0) {$j$};
			\node [neuron] (bias1) at (0,1) {$1$};
			\node []       (z1)      at (1,-.4) {$x^{(l-1)}_i$};
			\node []       (w)     at (3,-.4) {$w^{(l)}_{ij}$};
			\node []       (w)     at (3,.9) {$w^{(l)}_{0j}$};
			\node []       (aw)   at (5,-.4) {$a^{(l)}_j$};
			\node []	   (empty) at (8,0) {};
			\node []       (z2)    at (7,-.4) {$x^{(l)}_j$};
		
			\draw (sigma1) edge (sigma2);
			\draw (sigma2) edge (empty);
			\draw (bias1) edge (sigma2);
	\end{tikzpicture}
	\caption{A visual representation of the connections between unit $i$ in layer $l-1$, the bias unit in $l-1$, and unit $j$ in layer $l$. The connection strength between these units is given by the weight $w^{(l)}_{ij}$ between $i$ and $j$, and $w^{(l)}_{0j}$ between the bias unit and $j$. The activation $a^{(l)}_j$ at unit $j$ is computed by $a^{(l)}_j = w^{(l)}_{ij}x^{(l)}_i + w^{(l)}_0$. The output $x^{(l)}_j$ of unit $j$ is given by $x^{(l)}_j = \sigma(a^{(l)}_j)$}
	\label{connection}
\end{figure}
\noindent
Keeping track of the indices $l$, $i$ and $j$ quickly becomes confusing. By collecting all of the weights of connections going into unit $j$ in layer $l$ in a vector $\mathbf{w}^{(l)}_j$, the activation at unit $j$ can be computed as a dot product $a^{(l)}_j = {\mathbf{w}^{(l)}_j} \cdot \mathbf{x}^{(l-1)}$. Moreover, we can compute the entire vector $\mathbf{a}^{(l)}$ of activations at layer $l$, by organising the weight vectors $\mathbf{w}^{(l)}_j$ in a matrix $\mathbf{W}^{(l)} = \begin{bmatrix} \mathbf{w}^{(l)}_1 & \dots & \mathbf{w}^{(l)}_{d^{(l)}} \end{bmatrix}^T$, which leads to $\mathbf{a}^{(l)} = \mathbf{W}^{(l)}\mathbf{x}^{(l-1)}$.
\\\\
By gathering the weights in matrices $\mathbf{W}^{(l)}$, we have simplified our view of $h$ into a composition of matrix-vector products and element-wise application of activation functions. Figure \ref{neural_network} shows the parallel views of neural networks as networks of units and matrix-vector operations.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->, >=stealth, swap]
		\node [neuron] (bias0)   at (0, 2)  {1};
		\node [neuron] (x1)      at (0, 1)  {$x_1$};
		\node [neuron] (x2)      at (0, 0)  {$x_2$};
		\node []       (x)       at (0,-1)  {$\mathbf{x}^{(0)}$};
		\node [neuron] (bias1)   at (4, 2)  {1}; 
		\node [neuron] (sigma11) at (4, 1)  {$\sigma$};
		\node [neuron] (sigma12) at (4, 0)  {$\sigma$};
		\node []       (layer1)  at (4,-1)  {$\sigma(\mathbf{W}^{(1)} \mathbf{x}^{(0)}) = \mathbf{x}^{(1)}$};
		\node [neuron] (sigma21) at (8, 1)  {$\sigma$};
		\node [neuron] (sigma22) at (8, 0)  {$\sigma$};
		\node []       (layer2)  at (8,-1)  {$\sigma(\mathbf{W}^{(2)} \mathbf{x}^{(1)}) = h(\mathbf{x})$};
		\node []	   (empty1)  at (9, 1) {};
		\node []       (empty2)  at (9, 0) {};   
		
		
		\draw (bias0)   edge (sigma11);
		\draw (x1)      edge (sigma11);
		\draw (x2)      edge (sigma11);
		\draw (bias0)   edge (sigma12);
		\draw (x1)      edge (sigma12);
		\draw (x2)      edge (sigma12);
		\draw (bias1)   edge (sigma21);
		\draw (sigma11) edge (sigma21);
		\draw (sigma12) edge (sigma21);
		\draw (bias1)   edge (sigma22);
		\draw (sigma11) edge (sigma22);
		\draw (sigma12) edge (sigma22);
		\draw (sigma21) edge (empty1);
		\draw (sigma22) edge (empty2);
	\end{tikzpicture}
	\caption{A visual representation of $h(\mathbf{x}) = f_2(f_1(\mathbf{x}^{(0)}))$. The activation at each layer $\mathbf{a}^{(l)}$ is computed by $\mathbf{W}^{(l)}\mathbf{x}^{(l-1)}$. The output at each layer is computed by element-wise application of the activation function of $\sigma(\mathbf{a}^{(l)})$.}
	\label{neural_network}
\end{figure}
\noindent
We now have all the components we need to specify $\mathcal{H}$ as a set of neural networks. The set is defined by the depth of the networks $L$, the number of units in each layer $d_l$ , and the activation function $\sigma$.
For a particular $L$, $d_l$, and $\sigma$, each $h \in \mathcal{H}$ corresponds exactly to a unique assignment of real numbers to all of its weights. We can make the dependence of $h$ on its weights explicit by defining a vector $\mathbf{w} = \begin{bmatrix} w^{(1)}_{ij} & \dots & w^{(L)}_{ij}\end{bmatrix}$ and writing $h(\mathbf{x}, \mathbf{w})$ which means \textit{the function $h$ parameterised by the weight vector $\mathbf{w}$}. In the next section we discuss how to choose the activation functions at the layers of the network.

\subsection{Activation Functions}
\label{activation_functions}
Activation functions mimic the behaviour of neurons in the brain. A neuron emits a signal when the combined input it receives from other neurons exceeds a certain threshold. Activation functions achieve this by a variation of the step function, where an activation signal $a^{(l)}_j$ below the threshold is mapped to a value near zero, and an activation signal above the threshold is mapped to a value greater than zero. From a mathematical perspective, the role of activation functions is to introduce non-linearity in $h$, which allows it to approximate a much larger class of functions.
\\\\
Many networks use \textbf{sigmoid} activation functions such as the classical sigmoid function $\sigma(a) = \frac{1}{1 + \me^-a}$. These functions have the advantage of being differentiable everywhere. As we will see in section \ref{learning_algorithm}, differential calculus is the fundamental tool for finding a good $h \in \mathcal{H}$, which makes differentiability a desirable quality. One drawback of sigmoid activation functions is that their derivates are small, as seen in figure \ref{sigmoid}. For example, we can show that $\max \frac{d\sigma}{da} = \frac{1}{4}$. As we will see in section \ref{learning_algorithm}, neural networks are trained by multiplying chains of derivatives. When these derivatives are smaller than 1, the magnitude of the derivative shrinks in the length of the chain of terms which can make learning from $\mathcal{D}_{train}$ extremely slow.
\begin{figure}
	\centering
	\input{img/sigmoid.pgf}
	\caption{Sigmoid activation and its derivate. Sigmoid activation units have the disadvantage of \textbf{saturating}, meaning that they become flat when $a$ is large or small. This makes the derivative smaller than 1 everywhere, and much smaller than 1 almost everywhere.}
	\label{sigmoid}
\end{figure}
\\\\
Because of this shrinking problem, the default recommendation today is to use \textbf{rectified linear units}, depicted in figure \ref{relu}. These units use the activation function $\sigma(a) = \max(0, a)$. This function has the advantage that its derivative $\frac{d\sigma}{da} = 1$ when $a > 0$, and $\frac{d\sigma}{da} = 0$ when $a < 0$. This activation function is not strictly differentiable when $a = 0$. In practice however, this is not a big problem because $a$ is rarely exactly 0, and we may arbitrarily choose $\sigma(0)$ to be either 0 or 1, and still successfully train our networks.
\begin{figure}
	\centering
	\input{img/relu.pgf}
	\caption{ReLU activation and its derivate. Unlike sigmoid activation, ReLU activation doesn't saturate. This means that the derivative of a unit remains large whenever that unit produces output.}
	\label{relu}
\end{figure}
\\\\
Often we would like the output of $h$ to be a probability distribution over values in the label space $\mathcal{Y}$, since this makes it possible to design the so called \textbf{cost functions} with a principled technique \textbf{maximum likelihood}. For this reason it's common to use different activation functions in the output layer. 

For example, named entity recognition can be seen as a multi-class classification problem, where each token in a sentence must be assigned one of a fixed set of $C$ labels. To frame this as a probabilistic problem, we can encode each token label $\mathbf{y}$ as a vector of $C$ probabilities such that component $y_c$ of $\mathbf{y}_i \in \mathcal{D}_{train}$ is equal to 1 if $\mathbf{x}_i \in \mathcal{D}_{train}$ belongs to class $c$. All other components $y_{j\neq c}$ in $\mathbf{y}_i$ are equal to 0. This is known as \textbf{one-hot} encoding. $\mathbf{y}$ can be seen as a conditional probability distribution over each possible label given $\mathbf{x}_i$, that places all of the probability mass on label $c$.
\\\\
With one hot encoding, we can design $h$ to output vector with $C$ components, where each component $c \in h$ gives the probability that $\mathbf{x}$ has class $c$. More formally, we can interpret $h(\mathbf{x})$ as conditional probability distribution $h(\mathbf{x})_c = P(y = c | \mathbf{x})$.
\\\\
This type of output can be achieved by using the so-called \textbf{soft-max} activation function in the output layer. The soft-max activation is given by 
$$
\sigma(\mathbf{a})_c = \frac{\me^{a_c}}{\sum_{i = 1}^C \me^{a_i}}
$$ 
In words, the soft-max function makes sure that the output of $h$ is a valid probability distribution, firstly by making sure that each component of $h(\mathbf{x})$ is positive by taking the exponent, and by making sure that $\sum_{c=1}^Ch(\mathbf{x})_c = 1$ by dividing by the sum of all the exponentiated components. The last point means that unlike the other activation functions we have seen in this section, the soft-max must receive as input the vector $\mathbf{a}_L$ of all activations in layer $L$.
\\\\
Having designed the output layer of $h$ so that we can interpret its output as a conditional probability distribution, we can define the training error $\hat{E}(h, \mathcal{D}_{train})$, sometimes also called the \textbf{objective function} by the maximum likelihood principle, that quantifies the appropriateness of a weight vector $\mathbf{w}$ as a probability using the samples in $\mathcal{D}_{train}$. This function is crucial for finding $g \in \mathcal{H}$. We study it in the next section.

\subsection{Objective Function}
\label{objectiveFunction}
We would like a function that lets us compare functions in $\mathcal{H}$ in terms of how well they predict the samples in $\mathcal{D}_{train}$. Such a function is often called an objective function, borrowing terminology from the mathematical field of optimisation.
\\\\
In section \ref{activation_functions} we saw that the combination of one-hot encoding of the labels in $\mathcal{Y}$ and soft-max activation in the output layer of $h$ allows us to interpret $h(\mathbf{x})$ as a conditional probability distribution. In the following, we will use a convenient rewrite of the formula given in \ref{activation_functions}:
$$
P(y \mid \mathbf{x}) = \prod\limits_{c=1}^C h(\mathbf{x},\mathbf{w})^{y_c}_c
$$
Where $y$ is the true label for $\mathbf{x}_i$ and $y_c$ is component c of the one-hot vector $\mathbf{y}$. 
This formulation works because $\mathbf{y}$ is a one-hot vector, which means exactly one component of $\mathbf{y}$ is equal to 1, and all other components are equal to 0. So if $\mathbf{y} = \begin{bmatrix}0 & 1 & 0 \end{bmatrix}^T$ and $h(\mathbf{x}) = \begin{bmatrix}.1 & .8 & .1 \end{bmatrix}^T$, then $P(y \mid \mathbf{x}) = (0.1^0)(0.8^1)(0.1^0) = 0.8$.
\\\\
If we design $\mathcal{H}$ in such a way that every $h$ outputs a probability, we can use the principle of maximum likelihood to derive a plausible objective function. Maximum likelihood estimation uses the likelihood function to compute the probability of $\mathcal{D}_{train}$ by interpreting $h$ as a probability distribution parameterised by $\mathbf{w}$:

\begin{definition}[likelihood function]
	\label{likelihood}
	Let $\mathcal{D}_{train} = \{(\mathbf{x}_i, \mathbf{y}_i)\}$ be a set of $N$ training examples, where each $\mathbf{y}_i$ is a $C$ dimensional one-hot vector. Let $h(\mathbf{x}, \mathbf{w})$ be a neural network which outputs conditional probability distributions over the $C$ possible classes, such that $\sum_{c=1}^C h(\mathbf{x}, \mathbf{w})_c = 1$ and $0 \leq c \leq 1 \,\forall c \in h(\mathbf{x}, \mathbf{w})$. Furthermore, let the notation $y_{ic}$ denote component $c$ of the one-hot label for example $i$. Then the likelihood $P(\mathcal{D}_{train} \mid \mathbf{w})$ is:
	$$
	P(\mathcal{D}_{train} \mid \mathbf{w}) = \prod\limits_{i=1}^N\prod\limits_{c=1}^C h(\mathbf{x}_i, \mathbf{w})_c^{y_{ic}}
	$$
\end{definition}
\noindent
Informally, we can think of the likelihood function as asking the question, \textit{assuming that $h(\mathbf{x})$ is the true distribution from which $\mathcal{D}_{train}$ was sampled, what is the probability of observing the samples in $\mathcal{D}_{train}$?} Using the likelihood function to find a good $h \in \mathcal{H}$ is a matter of finding a weight vector $\mathbf{w}$ that maximise the likelihood of observing $\mathcal{D}_{train}$.
\\\\ 
Computing a large number of products of probabilities on a computer can be problematic because of \textbf{numerical underflow}. Since computers have limited precision, small positive numbers may be actually be represented as small negative numbers, which is bad because the likelihood function is a probability.
\\\\
To avoid numerical underflow, the \textbf{log-likelihood} $\ln P(\mathcal{D}_{train} \mid \mathbf{w})$ is often used instead. The logarithm turns the products into sums, which are entirely unproblematic for computers. Since the natural logarithm is a monotonic function, applying it to the likelihood function does not change the properties we are interested in.
\\\\
Finally, most other objective functions for supervised machine learning are defined in terms of training error $\trerror{h}$. In this view, searching for a good $h \in \mathcal{H}$ becomes a minimisation problem. For consistency, maximum likelihood estimation is often turned into a minimisation problem by using the \textbf{negative log-likelihood} $-\ln P(\mathcal{D}_{train} \mid \mathcal{W})$. In addition, most error measures are invariant to dataset size which makes it easy to compare the performance of a model on different data sets. To give the negative log-likelihood this property, it's common to divide by $N$, giving what is called the \textbf{average negative log-likelihood}. Minimising the average negative log-likelihood is clearly identical to maximising the likelihood, since $\max f(\mathbf{x}) = \min -f(\mathbf{x})$, and dividing by $N$ doesn't change the optimum.

\begin{definition}[average negative log-likelihood]
	\label{negative_log-likelihood}
	Let $\mathcal{D}_{train}$ and $h(\mathbf{x}; \mathcal{W})$ be defined as in definition \ref{likelihood}. Then the negative log likelihood $-\ln P(\mathcal{D}_{train} \mid \mathcal{W})$ is:
	$$
	\hat{E}(\mathbf{w}, \mathcal{D}_{train}) = - \frac{1}{N}\ln P(\mathcal{D}_{train} \mid \mathbf{w}) = - \frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{c=1}^C y_{ic} \ln h(\mathbf{x}_i, \mathbf{w})_c
	$$
\end{definition}
This error measure is also known as \textbf{cross-entropy error}, in which the term \\$-\sum_{c=1}^C y_{ic} \ln h(\mathbf{x}_i, \mathbf{w})_c$ is taken as the error measure $e(h(\mathbf{x}_i), \mathbf{y}_i)$, which allows us to write $\hat{E}$ in the familiar form used in section \ref{statistical_learning_theory}: $\hat{E}(h, \mathcal{D}_{train}) = \frac{1}{N}\sum_{i=1}^N e(h(\mathbf{x}_i), \mathbf{y}_i)$.

In the next section, we will see how to use the average negative log-likelihood to find a good $h \in \mathcal{H}$.
