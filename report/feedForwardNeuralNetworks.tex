\section{Feed-Forward Neural Networks}
A feed-forward neural network is a function $h: \mathcal{X} \mapsto \mathcal{Y}$. To understand how it works, it's instructive to look at each part of its name in isolation.
\\\\
$h$ is called a \textbf{network} because it's a composition of $L$ \textbf{layers} of other functions $f^{(l)}$. Each function $f^{(l)}$ receives input from $f^{(l-1)}$. For example if $L = 2$, then $h(\mathbf{x}) = f^{(2)}(f^{(1)}(\mathbf{x}))$. Each $f^{(l)}$ outputs a vector $\mathbf{x}^{(l)}$ of dimension $d^{(l)}$.  We denote the input to $f^{(1)}$ as $\mathbf{x}^{(0)}$ which is identical to the input vector $\mathbf{x}$ with an added \textbf{bias} component as described later in this section. The dimensionality of these vectors determine the \textbf{width} of the network. The number of layers $L$ is called the \textbf{depth} of the network. $f^{(L)}$ is called the \textbf{output layer}. The remaining functions $f^{(1)}$ to $f^{(L-1)}$ are called \textbf{hidden layers}. 
\\\\
The functions $f^{(1)}$ to $f^{(L)}$ are ordered by their index $l$ such that the index of the layers increase as we move from the input to the output layer. $h$ is called a \textbf{feed-forward} network because each $f^{(l)}$ can receive input only from functions $f^{(i)}$ if $l > i$. In other words, it's not possible for a function $f^{(l)}$ to feed its own output into itself, or any other function that it receives input from.
\\\\
Finally, $h$ is called a \textbf{neural} network since its design is loosely based on neurons in the brain \citep{goodfellow16}. Each component $x_i$ of the vector $\mathbf{x}^{(l)}$ can be seen as the output of a unit similar to a neuron. Each unit in layer $l$ receives input from units in layer $l-1$. The output $x^{(l-1)}_i$ of unit $i$ in layer $l-1$ is multiplied by a weight $w^{(l)}_{ij}$ that gives the strength of the connection between unit $i$ in $l-1$ and unit $j$ in $l$. Unit $j$ sums all of the input it receives from units in layer $l-1$ to obtain its \textbf{activation} $a^{(l)}_j = \sum_{i=0}^{d^{(l-1)}} w^{(l)}_{ij}x^{(l-1)}_{i}$. To compute its output $x^{(l)}_j$, it applies an \textbf{activation function} $\sigma(a^{(l)}_j)$ to the sum of its weighted input.

Activation functions model the behavior of biological neurons by outputting a signal only when the activation is above a certain threshold. To make it possible to learn this threshold for each unit using the same activation function, we introduce a special \textbf{bias} unit that always outputs 1. The index of the bias unit in layer $l$ is 0 by convention. Figure \ref{connection}. shows how a unit $j$ computes its output $x^{(l)}_j$ by combining the outputs of units in layer $l-1$.

\tikzstyle{neuron}=[circle,draw,minimum size=20pt,inner sep=0pt]
\tikzstyle{summation} = [square, minimum size=20pt,inner sep=0pt]
\tikzstyle{edge} = [draw,thick,-]
\tikzstyle{weight} = [font=\small]

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->, >=stealth, swap]
			\node [neuron] (sigma1) at (0,0) {$i$};
			\node [neuron] (sigma2) at (6,0) {$j$};
			\node [neuron] (bias1) at (0,1) {$1$};
			\node []       (z1)      at (1,-.4) {$x^{(l-1)}_i$};
			\node []       (w)     at (3,-.4) {$w^{(l)}_{ij}$};
			\node []       (w)     at (3,.9) {$w^{(l)}_{0j}$};
			\node []       (aw)   at (5,-.4) {$a^{(l)}_j$};
			\node []	   (empty) at (8,0) {};
			\node []       (z2)    at (7,-.4) {$x^{(l)}_j$};
		
			\draw (sigma1) edge (sigma2);
			\draw (sigma2) edge (empty);
			\draw (bias1) edge (sigma2);
	\end{tikzpicture}
	\caption{A visual representation of the connections between unit $i$ in layer $l-1$, the bias unit in $l-1$, and unit $j$ in layer $l$. The connection strength between these units is given by the weight $w^{(l)}_{ij}$ between $i$ and $j$, and $w^{(l)}_{0j}$ between the bias unit and $j$. The activation $a^{(l)}_j$ at unit $j$ is computed by $a^{(l)}_j = w^{(l)}_{ij}x^{(l)}_i + w^{(l)}_0$. The output $x^{(l)}_j$ of unit $j$ is given by $x^{(l)}_j = \sigma(a^{(l)}_j)$}
	\label{connection}
\end{figure}
\noindent
Keeping track of the indices $l$, $i$ and $j$ quickly becomes confusing. By collecting all of the weights of connections going into unit $j$ in layer $l$ in a vector $\mathbf{w}^{(l)}_j$, the activation at unit $j$ can be computed as a dot product $a^{(l)}_j = {\mathbf{w}^{(l)}_j} \cdot \mathbf{x}^{(l-1)}$. Moreover, we can compute the entire vector $\mathbf{a}^{(l)}$ of activations at layer $l$, by organising the weight vectors $\mathbf{w}^{(l)}_j$ in a matrix $\mathbf{W}^{(l)} = \begin{bmatrix} \mathbf{w}^{(l)}_1 & \dots & \mathbf{w}^{(l)}_{d^{(l)}} \end{bmatrix}^T$, which leads to $\mathbf{a}^{(l)} = \mathbf{W}^{(l)}\mathbf{x}^{(l-1)}$.
\\\\
By gathering the weights in matrices $\mathbf{W}^{(l)}$, we have simplified our view of $h$ into a composition of matrix-vector products and element-wise application of activation functions. Figure \ref{neural_network} shows the parallel views of neural networks as networks of units and matrix-vector operations.
\\\\
We now have all the components we need to specify $\mathcal{H}$ as a set of neural networks. The set is defined by the depth of the networks $L$, the number of units in each layer $d_l$ and the activation function $\sigma$.
For a particular $L$, $d_l$, and $\sigma$, each $h \in \mathcal{H}$ corresponds exactly to a unique assignment of real numbers to all of its weights. We can make the dependence of $h$ on its weights explicit by defining a vector $\mathbf{w} = \begin{bmatrix} w^{(1)}_{ij} & \dots & w^{(L)}_{ij}\end{bmatrix}$ and writing $h(\mathbf{x}, \mathbf{w})$ which means \textit{the function $h$ parameterised by the weight vector $\mathbf{w}$}. In practice, it's common to use different activation functions at different layers of the network. In the next section we discuss how to choose these activation functions.
\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->, >=stealth, swap]
		\node [neuron] (bias0)   at (0, 2)  {1};
		\node [neuron] (x1)      at (0, 1)  {$x_1$};
		\node [neuron] (x2)      at (0, 0)  {$x_2$};
		\node []       (x)       at (0,-1)  {$\mathbf{x}^{(0)}$};
		\node [neuron] (bias1)   at (4, 2)  {1}; 
		\node [neuron] (sigma11) at (4, 1)  {$\sigma$};
		\node [neuron] (sigma12) at (4, 0)  {$\sigma$};
		\node []       (layer1)  at (4,-1)  {$\sigma(\mathbf{W}^{(1)} \mathbf{x}^{(0)}) = \mathbf{x}^{(1)}$};
		\node [neuron] (sigma21) at (8, 1)  {$\sigma$};
		\node [neuron] (sigma22) at (8, 0)  {$\sigma$};
		\node []       (layer2)  at (8,-1)  {$\sigma(\mathbf{W}^{(2)} \mathbf{x}^{(1)}) = h(\mathbf{x})$};
		\node []	   (empty1)  at (9, 1) {};
		\node []       (empty2)  at (9, 0) {};   
		
		
		\draw (bias0)   edge (sigma11);
		\draw (x1)      edge (sigma11);
		\draw (x2)      edge (sigma11);
		\draw (bias0)   edge (sigma12);
		\draw (x1)      edge (sigma12);
		\draw (x2)      edge (sigma12);
		\draw (bias1)   edge (sigma21);
		\draw (sigma11) edge (sigma21);
		\draw (sigma12) edge (sigma21);
		\draw (bias1)   edge (sigma22);
		\draw (sigma11) edge (sigma22);
		\draw (sigma12) edge (sigma22);
		\draw (sigma21) edge (empty1);
		\draw (sigma22) edge (empty2);
	\end{tikzpicture}
	\caption{A visual representation of $h = f_2(f_1(\mathbf{x}^{(0)}))$. The activation at each layer $\mathbf{a}^{(l)}$ is computed by $\mathbf{W}^{(l)}\mathbf{x}^{(l-1)}$. The output at each layer is computed by element-wise application of the activation function of $\sigma(\mathbf{a}^{(l)})$.}
	\label{neural_network}
\end{figure}

\subsection{Activation Functions}
\label{activation_functions}
Activation functions mimic the behaviour of neurons in the brain. A neuron emits a signal when the combined input it receives from other neurons exceeds a certain threshold. Activation functions achieve this by a variation of the step function, where an activation signal $a^{(l)}_j$ below the threshold is mapped to a value near zero and an activation signal above the threshold is mapped to a value greater than zero. From a mathematical perspective the role of activation functions is to introduce non-linearity in $h$ which allows $\mathcal{H}$ to model a larger class of functions \citep{goodfellow16}.
\\\\
Many networks use \textbf{sigmoidal} activation functions such as the classical sigmoid function $\sigma(a) = 1/(1 + \me^{-a})$. These functions have the advantage of being differentiable everywhere. As we will see in section \ref{learningAlgorithm}, differential calculus is the fundamental tool for finding a good $h \in \mathcal{H}$ which makes differentiability a desirable quality. One drawback of sigmoidal activation functions is that their derivates are small as seen in figure \ref{sigmoid}. As we will see in section \ref{learningAlgorithm}, neural networks are trained by multiplying chains of derivatives. When these derivatives are smaller than 1, the magnitude of the derivative shrinks in the length of the chain of terms which can make learning from $\mathcal{D}$ extremely slow.
\begin{figure}
	\centering
	\input{img/sigmoid.pgf}
	\caption{Sigmoid activation and its derivate. Sigmoid activation units have the disadvantage of \textbf{saturating}, meaning that they become flat when $a$ is large or small. This makes the derivative smaller than 1 everywhere, and much smaller than 1 almost everywhere.}
	\label{sigmoid}
\end{figure}
\\\\
Because of this shrinking problem, the default recommendation today is to use \textbf{rectified linear units}. These units use the rectified linear activation function $\sigma(a) = \max(0, a)$ depicted in figure \ref{relu}. The rectified linear activation function has the advantage that its derivative $\frac{d\sigma}{da} = 1$ when $a > 0$, and $\frac{d\sigma}{da} = 0$ when $a < 0$. The function is not strictly differentiable when $a = 0$. In practice however, this is not a big problem because $a$ is rarely exactly 0 and since neural networks are trained through an iterative process as described in section \ref{gradient_descent} in which we can skip iterations where units have zero activation.
\begin{figure}
	\centering
	\input{img/relu.pgf}
	\caption{ReLU activation and its derivate. Unlike sigmoid activation, ReLU activation doesn't saturate. This means that the derivative of a unit remains large whenever it produces output.}
	\label{relu}
\end{figure}
\\\\
Often we would like the output of $h$ to be a probability distribution over values in the label space $\mathcal{Y}$ since this makes it possible to design the learning algorithm with a principled technique called \textbf{maximum likelihood} in which the appropriateness of $h$ is measured by the probability it assigns to the training data. For this reason, it's common to use different activation functions in the output layer that enables us to interpret the output of $h$ as a probability distribution. 
\\\\
For example, named entity recognition can be seen as a multi-class classification problem where each token in a sentence must be assigned one of a fixed set of $C$ labels. To frame this as a probabilistic problem we can encode each token label $\mathbf{y}$ as a vector of $C$ probabilities such that component $y_c$ of $\mathbf{y}_i$ is equal to 1 if example $\mathbf{x}_i$ belongs to class $c$. All other components $y_{j\neq c}$ in $\mathbf{y}_i$ are equal to 0. This is known as \textbf{one-hot} encoding. $\mathbf{y}_i$ can be seen as a conditional probability distribution over each possible label given $\mathbf{x}_i$ that places all of the probability mass on label $c$.
\\\\
Using one hot encoding we can design $h$ to output a vector with $C$ components where each component $h_c \in h(\vector{x}_i, \vector{w})$ gives the probability that $\mathbf{x}_i$ has class $c$ when $h$ is parameterized by $\vector{w}$. More formally, we can interpret $h(\mathbf{x})$ as conditional probability distribution such that $h(\mathbf{x})_c = P(Y = c \mid X = \mathbf{x})$ where $X$ and $Y$ are random variables over $\mathcal{X}$ and $\mathcal{Y}$.
\\\\
This type of output can be achieved by using the so-called \textbf{soft-max} activation function in the output layer of a neural network. The soft-max activation is given by 
$$
\sigma(\mathbf{a})_c = \frac{\me^{\vector{a}_c}}{\sum_{i = 1}^C \me^{\vector{a}_i}}
$$ 
Where the notation $\vector{a}_c$ denotes the $c$'th component of the vector $\vector{a}$. In words, the soft-max function ensures that the output of $h$ is a valid probability distribution by making sure that each component of $h(\mathbf{x})$ is positive by taking the exponent, and by making sure that $\sum_{c=1}^Ch(\mathbf{x})_c = 1$ by dividing by the sum of all the exponentiated components. The latter means that unlike the other activation functions we have seen in this section, the soft-max must receive as input the vector $\mathbf{a}^{(L)}$ of all activations in layer $L$.
\\\\
Having designed the output layer of $h$ so that we can interpret its output as a conditional probability distribution we can define the so called \textbf{objective function} by the maximum likelihood principle that quantifies the appropriateness of a weight vector $\mathbf{w}$ as a probability using the samples in $\mathcal{D}$. This function is crucial for finding $g \in \mathcal{H}$.

\subsection{Objective Function}
\label{objectiveFunction}
We would like a function that lets us compare functions in $\mathcal{H}$ in terms of how well they predict the samples in $\mathcal{D}$. Such a function is often called an objective function, borrowing terminology from the mathematical field of optimization.
\\\\
In section \ref{activation_functions} we saw that the combination of one-hot encoding of the labels in $\mathcal{Y}$ and soft-max activation in the output layer of $h$ allows us to interpret $h(\mathbf{x})$ as a conditional probability distribution. In the following, we will use a convenient rewrite of the formula given in \ref{activation_functions}:
$$
P(Y = y \mid X = \mathbf{x}) = \prod\limits_{c=1}^C h(\mathbf{x},\mathbf{w})^{\vector{y}_c}_c
$$
Where $y$ is the true label for $\mathbf{x}_i$ and $\vector{y}_c$ is component c of the one-hot vector $\mathbf{y}$. 
This formulation works because $\mathbf{y}$ is a one-hot vector, which means exactly one component of $\mathbf{y}$ is equal to 1, and all other components are equal to 0. So if $\mathbf{y} = \begin{bmatrix}0 & 1 & 0 \end{bmatrix}^T$ and $h(\mathbf{x}) = \begin{bmatrix}.1 & .8 & .1 \end{bmatrix}^T$, then $P(Y = y \mid X = \mathbf{x}) = (0.1^0)(0.8^1)(0.1^0) = 0.8$.
\\\\
If we design $\mathcal{H}$ in such a way that every $h$ outputs a probability, we can use the principle of maximum likelihood to derive a plausible objective function. Maximum likelihood estimation uses the likelihood function to compute the probability of $\mathcal{D}$ by interpreting $h$ as a probability distribution parameterized by $\mathbf{w}$:

\begin{definition}[likelihood function]
	\label{likelihood}
	Let $\mathcal{D} = \{(\mathbf{x}_i, \mathbf{y}_i)\}$ be a set of $N$ training examples, where each $\mathbf{y}_i$ is a $C$ dimensional one-hot vector. Let $h(\mathbf{x}, \mathbf{w})$ be a neural network which outputs conditional probability distributions over the $C$ possible classes, such that $\sum_{c=1}^C h(\mathbf{x}, \mathbf{w})_c = 1$ and $0 \leq c \leq 1 \,\forall c \in h(\mathbf{x}, \mathbf{w})$. Furthermore, let the notation $\vector{y}_{ic}$ denote component $c$ of the one-hot label for example $i$. Then the likelihood $P(\mathcal{D} \mid \mathbf{w})$ is:
	$$
	P(\mathcal{D} \mid \mathbf{w}) = \prod\limits_{i=1}^N\prod\limits_{c=1}^C h(\mathbf{x}_i, \mathbf{w})_c^{\vector{y}_{ic}}
	$$
\end{definition}
\noindent
Informally, we can think of the likelihood function as asking the question: \textit{assuming that $h(\mathbf{x})$ is the true conditional distribution from which $\mathcal{D}$ was sampled, what is the probability of observing the samples in $\mathcal{D}$?} Using the likelihood function to find a good $h \in \mathcal{H}$ is a matter of finding a weight vector $\mathbf{w}$ that maximize the likelihood of observing $\mathcal{D}$.
\\\\ 
Computing a large number of products of probabilities on a computer can be problematic because of \textbf{numerical underflow}. Since computers have limited precision, small positive numbers may be actually be represented as small negative numbers which may lead to problems because the likelihood function must be interpreted as a probability in neural network training.
\\\\
To avoid numerical underflow, the \textbf{log-likelihood} $\ln P(\mathcal{D} \mid \mathbf{w})$ is often used instead. The logarithm turns the products into sums, which are entirely unproblematic for computers. Since the natural logarithm is a monotonic function, applying it to the likelihood function does not change the properties we are interested in, namely it's maximum.
\\\\
Finally, many objective functions for supervised machine learning are defined in terms of training \emph{error} $\trerror{h}$ and not \emph{probability}. In this view, searching for a good $h \in \mathcal{H}$ becomes a minimization problem. For consistency, maximum likelihood estimation is often turned into a minimisation problem by using the \textbf{negative log-likelihood} $-\ln P(\mathcal{D}_{train} \mid \mathcal{W})$. In addition, most error measures are invariant to dataset size which makes it easy to compare the performance of a model on different data sets. To give the negative log-likelihood this property, it's common to divide by $N$, giving what is called the \textbf{average negative log-likelihood}. Minimizing the average negative log-likelihood is clearly identical to maximizing the likelihood, since $\max f(\mathbf{x}) = \min -f(\mathbf{x})$, and dividing by $N$ doesn't change the optimum.

\begin{definition}[average negative log-likelihood]
	\label{negative_log-likelihood}
	Let $\mathcal{D}$ and $h(\mathbf{x}; \mathcal{W})$ be defined as in definition \ref{likelihood}. Then the average negative log likelihood $-\ln P(\mathcal{D} \mid \mathcal{W})$ is:
	$$
	\hat{E}(\mathbf{w}, \mathcal{D}) = - \frac{1}{N}\ln P(\mathcal{D} \mid \mathbf{w}) = - \frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{c=1}^C y_{ic} \ln h(\mathbf{x}_i, \mathbf{w})_c
	$$
\end{definition}
\noindent
This error measure is also known as \textbf{cross-entropy error} in which the term \\$-\sum_{c=1}^C y_{ic} \ln h(\mathbf{x}_i, \mathbf{w})_c$ is taken as the error measure $e(h(\mathbf{x}_i), \mathbf{y}_i)$, which allows us to write $\hat{E}$ in the familiar form used in section \ref{statistical_learning_theory}: $\hat{E}(h, \mathcal{D}) = \frac{1}{N}\sum_{i=1}^N e(h(\mathbf{x}_i), \mathbf{y}_i)$.

In the next section, we will see how to use the average negative log-likelihood to find a good $h \in \mathcal{H}$.
