\section{Algorithm}

Our main goal is to investigate the sample complexity dynamics of learning a relation classification task in a multi-task learning setting when the data available for the target task is limited. To this end, we compare the generalisation error of a deep learning model trained only on SemEval 2010 Task 8, the target task, with the generalisation error of a deep learning model trained jointly on the SemEval data and one of the auxiliary tasks described in \ref{auxiliary_tasks}.
\\\\
We proceed as follows: We vary the amount of data from the target task and auxiliary task in turn by a set of fractions. For every combination of fractional target and auxiliary data, we perform 5-fold cross validation on the target data to yield 5 macro-F1 scores. We use the training data from the 4 training folds of target data and the auxiliary data to train the architecture described in \ref{network_architecture}. This is done by uniformly selecting one of the two tasks, sampling a mini-batch of the fractional training data from that task, and performing one gradient descent update with respect to cross-entropy error using the Adam algorithm described in section \ref{adam}. 
\\\\
This process is iterated until an early stopping criterion on the target training data is met. Specifically, $1/10$ of target training data is set aside for early stopping validation. When the cross-entropy error on the early stopping dataset has not improved for 200 iterations of mini-batch gradient descent, training is halted, and the model weights are reset to their best recorded value.

When the patience is exceeded we record the cross-validation macro-F1 on the target task test fold using the best recorded weights. Since neural network training is a random search procedure with respect to weight initialization and mini-batch sampling, we run this experiment for each combination of target and auxiliary fractional data 5 times, yielding a total of 25 random cross-validation splits for each combination. We have provided the algorithm used in our experiments as pseudocode in algorithm \ref{experiment_pseudocode}.
\begin{algorithm}
\begin{algorithmic}
	\Require Target dataset $\data_{target}$
	\Require Auxiliary dataset $\data_{aux}$
	\Require mini-batch size $b$
	\Function{crossValidation}{$\data$}
		\State \textbf{return} \textit{$K$ train and test cross validation folds.}
	\EndFunction
	\Function{sample}{set $S$ of size $N$, $f$}
		\State \textbf{return} \textit{$N \times f$ samples from $S$ sampled uniformly}
	\EndFunction
	\Function{initializeWeights}{ }
		\State \textbf{return} initialized neural network weight vector $\vector{w}$
	\EndFunction
	\Function{gradientDescent}{$\data$, $\vector{w}$}
		\State \textbf{return} \textit{the weight vector $\vector{u}$ resulting from a single gradient descent step on $\data$ with the weights $\vector{w}$ using the Adam algorithm}
	\EndFunction
	\Function{macroF1}{$\vector{w}$,$\data$}
		\State \textbf{return} \textit{Macro F1 of the neural network parameterized by $\vector{w}$ on $\data$}
	\EndFunction
	\Function{report}{$s$, $f$}
		\State \textit{report score $s$ and fraction $f$ to user}
	\EndFunction
	\ForAll{\textit{5 iterations}}
		\For{$f \in \{\frac{0}{5}, \frac{1}{5}, \frac{2}{5}, \frac{3}{5}, \frac{4}{5}, 1\}$}
			\For{$\data_{trainFold}, \data_{val} \in$ \Call{crossValidation}{$\data_{target}$}}
				\State $\data_{earlyStopping} \gets$ \Call{sample}{$\data_{trainFold}$, $\frac{1}{10}$}
				\State $\data_{train} \gets \data_{trainFold} \setminus \data_{earlyStopping}$
				\State $\data_{f} \gets $ \Call{sample}{$\data_{train}$, $f$}
				\State $\vector{w}_0 \gets$ \Call{initializeWeights}{ }
				\State $\vector{w}_{best} \gets \vector{w}_0$
				\State $i \gets 1$
				\While{\textit{patience not exceeded}}
					\State $\mathcal{T} \gets$ \Call{sample}{$\{\data_f, \data_{aux}\}$, $\frac{1}{2}$}
					\State $\mathcal{B} \gets$ \Call{sample}{$\mathcal{T}$, $\frac{|\mathcal{T}|}{b}$}
					\State $\vector{w}_i \gets$ \Call{gradientDescent}{$\mathcal{B}$, $\vector{w}_{i-1}$}
					\State $i \gets i + 1$
					\If{\textit{$\hat{E}(\vector{w}_i, \data_{earlyStopping})$ was the best recorded}}
						\State $\vector{w}_{best} \gets \vector{w}_i$
					\EndIf
				\EndWhile
				\State $s \gets$ \Call{macroF1}{$\vector{w}_{best}$, $\data_{val}$}
				\State \Call{report}{$s$,$f$}
			\EndFor
		\EndFor
	\EndFor
\end{algorithmic}
\caption{Pseudocode for our deep multi-task learning experiment.}
\label{experiment_pseudocode}
\end{algorithm}
