\section{Motivation}
Information available as digital text is rapidly increasing: The number of active science journals is growing with an annual rate of approximately 2.5\%. The total number of published articles in these journals was approximately 2.5 million in 2015, up from 1.8 in 2009 \citep{ware2009, ware2015}. Similar staggering growth rates can be cited for online newspapers, social media content and digital documents generated by businesses.
\\\\
Finding the right information at the right time in the face of this data volume is challenging. One of the main reasons is that information contained in digital text is natural language data, which is often cited as being \textbf{unstructed} (despite the fact that natural language is actually highly structured). Unstructured data can be understood as essentially the opposite of the kind of data we find relational databases for example, where every data item is associated with metadata such as column names and column types, which makes the structured data easy to search and analyze with computers. Unstructured data such as text, images, sound and video
 in contrast is not.
\\\\
This problem has driven the development of so called \textbf{information extraction} techniques. The goal of these is essentially to assign metadata to unstructured data, thereby giving some structure to it. This is an ambitious goal, since in many cases it involves developing computer systems that perform tasks such as recognizing objects in images, converting speech to text and building data structures that capture the semantics of natural language, tasks which we currently do not fully understand how humans are able to perform.
\\\\
\textbf{Supervised machine learning} in general, and \textbf{deep learning} in particular, is a very successful technique for solving information extraction problems. This approach is based on the idea of supplying examples of inputs such as text, and corresponding correct outputs, so called labels, that we would like the system to reproduce given the input, in the hope that the system can learn to give approximately correct output for new inputs that it hasn't seen before.
\\\\
The conditions under which supervised machine learning systems can be expected to give approximately correct answers are fairly well understood. One of the main ingredients in this guarantee is the number of training examples provided for the learning system. Producing these examples can be quite expensive however. It often requires a human annotator with specialized skills to provide the correct labels, for example a trained linguist or a doctor. This means there is significant motivation for reusing labeled training data, to reduce the need to create large new collections of data for each new supervised machine learning problem.
\\\\
\textbf{Multi-task learning} is a technique for reusing labeled data. In essence, it relies on the idea that it may be easier to learn related tasks simultaneously than in isolation. For a new target supervised machine learning problem, it may be the case that already annotated datasets for related auxiliary tasks exists, in which case learning from all the available data may reduce the cost of creating a new annotated dataset.