\section{Bias Learning}
\label{bias_learning}
Selecting the hypothesis space $\mathcal{H}$, sometimes referred to as \textbf{biasing} the hypothesis space, is often the hardest problem in supervised machine learning \citep{baxter2000}. Vapnik-Chervonenkis analysis tells us that $\mathcal{H}$ must be large enough to contain a good solution to the learning problem of interest, yet small enough that the selected model can generalize from a small sample. This motivates developing techniques that can learn a good $\hypspace$ from the data.
\\\\
\citet{baxter2000} formalizes this idea by introducing a model of \textbf{bias learning} in which the learning system is tasked with learning a hypothesis space $\mathcal{H}$ from a family of hypothesis spaces $\mathbb{H} = \{\mathcal{H}\}$. The system is supplied with $M$ datasets $\data_m$ each drawn from $M$ distributions $P_m$ over $\mathcal{X} \times \mathcal{Y}$. The goal of the system is then to first select a good hypothesis space $\mathcal{H} \in \mathbb{H}$, and then to select a vector $\vector{h}$ of $M$ hypothesis $h_m \in \mathcal{H}$. In his framework, the goal of the learning system is to minimize the multi-task generalization error defined as the average generalization error over the $M$ learning problems:
$$
E(\vector{h}) = \frac{1}{M}\sum\limits_{m = 1}^M E_m(h_m)
$$
Similarly, we can generalize the empirical single-task error to an average multi-task empirical error $\hat{E}(\vector{h}, \data_M)$: 
$$
\hat{E}(\vector{h}, \data_M) = \frac{1}{M}\sum\limits_{m = 1}^M\hat{E}(h_m, \data_m)
$$
The bias learning model of \citet{baxter2000} extends Vapnik-Chervonenkis analysis to the multi-task learning problem. To this end, he defines $\mathcal{H}(N, M)$ to be the set of all matrices of dichotomies, that can be formed from selecting $M$ hypothesis from $\mathcal{H}$ and applying them to the to the $N$ samples of the $M$ datasets in $\data_M$:
$$
\mathcal{H}(N, M) = \left\{ \begin{bmatrix}
	h_1(\vector{x}_{11}) & \cdots & h_1(\vector{x}_{1N}) \\
	\vdots & \ddots & \vdots \\
	h_M(\vector{x}_{M1}) & \cdots & h_M(\vector{x}_{MN})
\end{bmatrix}\, :\, h_1,\, \dots,\, h_M \in \mathcal{H} \right\}
$$

This allows him to define a concept of dichotomies on multi-task samples $\data_M$ for hypothesis space families, $\mathbb{H}(N, M)$:
$$
\mathbb{H}(N, M) = \bigcup\limits_{\mathcal{H} \in \mathbb{H}} \mathcal{H}(N, M)
$$
And extend the growth function $m$ to the multi-task setting:
$$
m(N, M, \mathbb{H}) = \max|\mathbb{H}(N, M)|
$$
With a binary label space, the maximum size of $\mathbb{H}(N, M)$ is $2^{NM}$. Baxter uses this to define the Vapnik-Chervonenkis dimension $d(M, \mathbb{H})$ of the hypothesis space family $\mathbb{H}$:
$$
d(M, \mathbb{H}) = \max\{N\, : \, m(N, M, \mathbb{H}) = 2^{NM}\}
$$
In words, the Vapnik-Chervonenkis dimension of the hypothesis space family $\mathbb{H}$ is the largest number of samples $N$ for $M$ tasks for which the family can generate all possible binary dichotomy matrices.
\\\\
Using the same reasoning as is the basis of the original Vapnik-Chervonenkis bound, Baxter is able to show that in order for the average true error $E(\vector{h})$, to be within $\epsilon$ of the average empirical error $\hat{E}(\vector{h}, \data_M)$ with probability $1 - \delta$, it requires that the number of samples $N$ for each task is:
$$
N \geq O\left(\frac{1}{\epsilon^2}\left(d(M, \mathbb{H}) \log \frac{1}{\epsilon} + \frac{1}{M} \log \frac{1}{\delta}\right)\right)
$$
Ignoring the confidence parameters $\epsilon$ and $\delta$, we see that the number of examples $N$ depends inversely on the number of tasks $M$. This means that we can reduce the number of samples required to keep $E$ close to $\hat{E}$ if we can increase the number of learning tasks. This is an important result since it shows that multi-task bias learning can improve our confidence that $E_m$ is close to $\hat{E}(h_m, \data_m)$ at least on average. 

On the other hand, it's also a limited result in the sense that it doesn't tell us anything about how $\hat{E}(h_m, \data_m)$ behaves in multi-task learning relative to single-task learning. In other words, it may be possible that bias learning leads to a hypothesis space $\mathcal{H}$ where $\hat{E}(\vector{h}, \data_M)$ is close to $E(\vector{h})$, but $E(\vector{h})$ is much larger than would have been possible to achieve if the learning algorithm applied to each task was not restricted to the same hypothesis space.