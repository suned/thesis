\section{Information Extraction}
\label{information_extraction}
In natural language processing, information extraction is the problem of extracting structured information from unstructured text. Many practical information extraction problems fall in one of two categories: \textbf{named entity recognition}, or \textbf{relation extraction} \citep{jurafsky09}.

\subsection{Named Entity Recognition}
\label{named_entity_recognition}
A named entity is roughly anything that has a proper name. The task in named entity recognition is to label mentions of entities such as people, organisations or places occurring in natural language. As an example, consider the sentence: 
$$
\text{Jim bought 300 shares of Acme Corp. in 2006.}
$$ 
A named entity recognition system designed to extract the entities \textit{person} and \textit{organisation} should ideally assign the labels:
$$
	[\text{Jim}]_{person} \text{ bought 300 shares of } [\text{Acme Corp.}]_{organisation} \text{ in 2006.}
$$
This is a difficult problem because of two types of ambiguity. Firstly, two distinct entities may share the same name and category, such as \textit{Francis Bacon} the painter and \textit{Francis Bacon} the philosopher. Secondly, two distinct entities can have the same name, but belong to different categories such as \textit{JFK} the former American president and \textit{JFK} the airport near New York.
\\\\
Named entity recognition can be framed as a sequence labeling problem. A common approach is to apply so called tokenisation to the text, i.e finding boundaries between words and punctuation, and associate each token with a label indicating which entity it belongs to. BIO (figure \ref{bio}) is a widely used labelling scheme in which token labels indicate whether the token is at the \textbf{B}eginning, \textbf{I}nside, or \textbf{O}utside an entity mention.
\begin{figure}
	\begin{center}
		\begin{tabular}{c c c c c c c c c c c}
	Jim & bought & 300 & shares & of & Acme & Corp & . & in & 2006 & . \\
	\texttt{B-PER} & \texttt{O} & \texttt{O} & \texttt{O} & \texttt{O} & \texttt{B-ORG} & \texttt{I-ORG} & \texttt{I-ORG} & \texttt{O} & \texttt{O} & \texttt{O}
	\end{tabular}
	\end{center}
	\caption{A sentence labeled with the BIO labels for named entity recognition.}
	\label{bio}
\end{figure}

\subsection{Relation Extraction}
\label{relation_extract}
Relation extraction refers to the problem of identifying relationships such as \textit{Family} or \textit{Employment} between entities. As an example, consider the sentence: 
$$
\text{Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp.}
$$ 
Imagine we have designed a relation extraction system that recognises the relation \textit{MergerBetween(organisation, organisation)} between two mentions of organisations. Ideally, we would like that system to extract the relation \textit{MergerBetween(Foo Inc., Bar Corp.)} from the above sentence.
\\\\
Because relation extraction is concerned with relationships between named entities, many systems that perform relation extraction applies named entity recognition first as a pre-processing step. This approach is sometimes called \textbf{pipelining}. An alternative to pipelining is \textbf{end-to-end} relation extraction, where relations and entities are extracted simultaneously. Pipeline approaches can suffer from the problem of \textbf{error propagation}, where the system erroneously assigns a label in the named entity recognition step, which later causes it to make an error in the relation extraction step.

\subsection{Accuracy Measures}
Information extraction systems are often evaluated empirically by applying them to collections of text, so called corpora, in which $N$ mentions of named entities or relations are known. Accuracy measures used in such tests are usually defined in terms of:
\begin{description}
	\item [True positives ($tp$)] The number of true named entities or relations correctly labeled by the system.
	\item [True negatives ($tn$)] The number of true non-entities or non-relations correctly labeled by the system.
	\item [False positives ($fp$)] The number of true non-entities or relations incorrectly labeled by the system.
	\item [False negatives ($fn$)] The number of true named entities or relations incorrectly labeled by the system.
\end{description}
The distribution of labels used in both named entity recognition and relation extraction is often highly imbalanced. Consider for example the BIO labelling scheme in figure \ref{bio}. Most words will be outside a mention of a named entity, and will have the label \texttt{O}. Using simple accuracy $\frac{tp + tn}{N}$ as a performance metric is therefore not very informative, since a useless system which labels all tokens with \texttt{O} would achieve high performance.

\textbf{Precision} and \textbf{recall} are more appropriate performance metrics for this reason. Precision $\frac{tp}{tp + fp}$ is the fraction of true named entities or relations of all named entities or relations that were extracted by the system. This is equal to 0 when none of the information extracted by the system was correct and 1 when all of it was correct. 

Recall $\frac{tp}{tp + fn}$ is the fraction of true named entities or relations that were extracted by the system. This is 0 when none of the extracted information was correct, and 1 when all of the extracted information was correct, and no true named entities or relations were incorrectly labeled.

To get a single number that summarises the performance, precision $p$ and recall $r$ are often combined into a single metric, the $F1$ measure, defined as the harmonic mean of precision and recall $\frac{2pr}{p + r}$.