\chapter{Discussion}
In the previous section we saw that all but one weight sharing strategy tested in our deep multi-task learning experiments did not lead to significant improvements in generalization error for relation classification. In this section, we reflect on this result in order to outline the conclusions we can draw.

\section{Impact of Limited Weight Sharing}
As discussed, the neural network architecture adopted from \citet{nguyen2015} puts certain limitations on how neural network weights can be shared between tasks in practice. The solution to this problem of sharing only the position and word embedding matrices was motivated by \citet{collobert2008}. They show that sharing the word vector weights of convolutional neural network architectures between auxiliary syntactic labeling tasks such as part-of-speech tagging and a target semantic role labeling task leads to significant improvements in generalization error for the target task.
\\\\
However, all the tasks that are learnt simultaneously in \citet{collobert2008} are derived from annotations of the same sections of the PropBank corpus \citep{kingsbury2002}. When annotations for auxiliary tasks are taken from different corpora, the potential benefits made possible by sharing only the word embedding is limited by the degree of overlap of words occurring in both corpora. The set of words that occur in a corpora is commonly referred to as it's \textbf{vocabulary}. The only way an auxiliary task can benefit the target task is if the weights that are updated while learning from an auxiliary task are also used by the target task. When sharing only the word embedding, this happens only when a word is in both the auxiliary vocabulary and in the target vocabulary.
\\\\
In \citet{collobert2008} the vocabulary overlap is maximal since all annotations for all tasks pertain to the same text. This is not the case for the corpora used in our experiments as seen in figure \ref{vocab_overlap}. We speculate that the reason we observe better results when sharing convolutional filter weights as compared to sharing just the embedding matrices is in large part due to the fact that the convolutional filters are guaranteed to be used by both tasks. Intuitively, one task may then benefit the other if the features detected by a filter learnt by one task is useful for the other.
\newpage
\thispagestyle{empty}
\begin{figure}
	\centering
	\input{img/ace_vocab_overlap.pgf}
	\input{img/conll_pos_vocab_overlap.pgf}
	\input{img/gmb_vocab_overlap.pgf}
	\caption{Vocabulary overlap between the corpora used in our experiments. The Venn diagrams show the number of tokens occurring in both the SemEval 2010 Task 8 vocabulary and each of the vocabularies of the auxiliary tasks.}
	\label{vocab_overlap}
\end{figure}
\FloatBarrier

\section{Semantic Relations are Inconsistently Defined}
The reasoning in the previous section does not explain why the multi-channel weight sharing strategy and sharing convolutional filters over the augmented sentence matrix when learning the ACE 2005 relation classification as an auxiliary task leads to different results. The differences in generalization performance between these two approaches indicate that there are few convolutional features of the un-augmented sentence matrix that are learnable from the ACE 2005 dataset that are useful for the target task.
\\\\
This result suggests that there must be general differences in patterns of semantic and syntactic information encoded in word vectors alone that are good predictors of relation types in the two datasets. This is surprising given that, superficially, there is a clear semantic overlap between some of the relations in the two tasks as argued in section \ref{target_task}. However, as pointed out in \citet{handschuh2016}, the objective of relation classification is ill-defined in the sense that the restrictions on what constitutes a valid relation varies from dataset to dataset. This leads to some important general differences between the relations found in the ACE 2005 relation classification task and SemEval 2010 Task 8.
\\\\
We have already discussed the definition of a valid relation enforced during the annotation process of the SemEval dataset in section \ref{target_task}. To reiterate, they were:
\begin{itemize}
	\item Relation arguments cannot depend on discourse knowledge (e.g they can't be pronouns).
	\item Relation arguments cannot appear in separate sentential clauses.
\end{itemize}
No such restrictions are present in the ACE 2005 annotation guidelines
\\\\
These differences between annotation guidelines lead to significant differences between the types of relations that appear in the two datasets. The sentence: 
\begin{quote}
\textit{The fifty essays collected in this volume testify to most of the prominent themes from Professor Quispel's scholarly career}	
\end{quote}
\noindent
is a canonical example of \textit{Member-Collection(essays, volume)} taken from the SemEval dataset. As typical for the SemEval dataset, the arguments belong to different noun phrases: \textit{the fifty essays} and \textit{this volume}. The two noun phrases are separated by words that are informative of the \textit{Member-Collection} relation, namely \textit{collected in}.
\\\\
Contrast this with the following canonical example of the \textit{Personal-Social(his, father-in-law)} relation from the ACE 2005 dataset: 
\begin{quote}
\textit{The fact that this guy was such an idiot to go back and let his father-in-law kill him shows he wasn't the most stable of people}.	
\end{quote}
 Here, the arguments belong to the same noun phrase \textit{his father-in-law}. We speculate that the kind of convolutional feature detectors that are useful for classifying relations where both relation arguments appear in the same noun phrase are not very useful for classifying relations where the arguments appear in separate noun phrase. This indicates that there are examples in the ACE 2005 dataset that requires the network to learn semantic and syntactic feature detectors that are unlikely to be useful for detecting canonical SemEval relations.
\\\\
If this syntactic and semantic pattern mismatch between the relations in the two datasets occur frequently enough, we speculate that it explains why learning ACE 2005 as an auxiliary task only leads to improvements in generalization performance when the convolutional filters over the augmented sentence matrix that also contains position information is shared between the target and auxiliary tasks. In this weight sharing scheme, the training examples in ACE 2005 that are more similar to the examples in SemEval can benefit the target network with more powerful feature detectors that incorporate position information.
\\\\
We can estimate the frequency of this syntactic pattern as follows: Construct a syntactic parse tree for the sentence in which $relation(arg1,arg2)$ occurs. Traverse the tree bottom-up from the leaves corresponding to $arg1$ and $arg2$ in turn. Record the first noun phrase node encountered between the leaves and the root. Denote this node as the nearest-ancestor noun phrase of the argument. Let the predicate $sameNP(arg1, arg2)$ be a logical predicate on the arguments of the relation that is true when they share a nearest-ancestor noun phrase node. Count the examples in each dataset for which $sameNP(arg1, arg2)$ is true.
\\\\
We have counted the number of samples for which $sameNP(arg1,arg2)$ is true for both the SemEval dataset and the ACE 2005 dataset using the Stanford PCFG parser \citep{klein2003}. In addition, we have counted the number of relations in which one of the arguments is a pronoun denoted by the predicate $pronoun(arg1,arg2)$. The results can be seen in figure \ref{same_noun_phrase}. Specifically we see that more than half of the samples in the ACE 2005 dataset are made up of sentences in which the relation arguments share a nearest-ancestor noun phrase node. In the SemEval dataset, there are hardly any samples of this sort.
\\\\
This leads to the following conclusion: the inconsistent requirements for what constitutes a valid relation in the two datasets as expressed by the annotation guidelines lead to samples in which the syntactic and semantic indicators of the presence of a semantic relation are very different. This inconsistency is not conductive for multi-task learning, since it makes it unlikely that the features that are useful for one task is also useful for the other. We have identified only two major differences in the general syntactic patterns of the relations between the two datasets. We speculate that a more thorough analysis would lead to more. This indicates that if we want to re-use annotated data for relation classification, it calls for the development of a general, unambiguous definition of relation classification as a task as is argued in \cite{handschuh2016}.
\newpage
\begin{figure}[h!]
	\centering
	\input{img/ace_syntactic_analysis.pgf}
	
	\vspace{1cm}
	
	\input{img/semeval_syntactic_analysis.pgf}
	\caption{Venn diagram of relations where the relation arguments are part the same noun phrase or one relation is a pronoun for the ACE 2005 dataset and the SemEval 2010 Task 8 dataset.}
	\label{same_noun_phrase}
\end{figure}
\noindent
The argument we have presented in this section is speculative in nature because it's difficult to inspect exactly what kinds of feature detectors a convolutional neural network for natural language processing tasks learn. In contrast, convolutional neural networks for image recognition tasks learn feature detectors that can very intuitively be seen as general edge or line detectors in the early layers, and more specialized object detectors in the the layers close to the output \citep{goodfellow16}. This makes it easy to reason about whether the early layers of networks tasked with learning two different things are learning similar feature detectors, which in turn makes it easier to predict whether sharing the weights of early layers between them can lead to generalization improvements. 

If a similar language for describing what is detected by convolutional filters in a neural network for a language processing task could be developed, this this would be a significant step towards answering the questions: \textit{which tasks are useful as auxiliary tasks?} and \textit{how should weights of the networks for two tasks be shared?}

\section{The Need for A Unifying Theory of Multi-Task Learning}
As discussed in section \ref{task_relatedness}, empirical experiments as those in this thesis are the most reliable way to investigate which auxiliary tasks to use and how neural network weights be shared in order for multi-task learning to improve generalization for a specific application. The lack of theoretical understanding presents us with a problem: When a deep multi-task learning experiment leads to a negative result it may be due to unfit auxiliary tasks, an unfit neural network architecture or both.
\\\\
The sequence classification tasks we have tested do not lead to generalization gains for any of the architectures that we have tested. Whether or not changing the neural network architecture can yield generalization gains by multi-task learning remains to be seen. The major question is: how much energy should we spend on experimenting with this and that architecture before we conclude that two tasks are unrelated? Or, if we do see generalization improvements on the target task, how can we know that our network architecture really takes advantage of all the useful information in the auxiliary tasks, and that further gains are not possible still?
\\\\
We believe a unified theory of multi-task learning that can provide answers to these types of questions is an important goal for future research. In particular, such a theory should provide us with statistical tests that can be applied to two tasks and indicate whether generalization gains should be possible by learning them simultaneously. This is an ambitious goal since it involves making predictions about the neural network features induced by a particular dataset. Nonetheless, we believe the potential for efficient development of multi-task learning systems with high performance that such a theory would make possible would be extremely valuable in both business and research.

\section{Summary}
In this section we have reflected on the results of our deep multi-task learning experiments. We have argued that when combining text data from disparate corpora, sharing only the word embedding is effective only to the extent that the vocabulary of those corpora overlap. 

Moreover, we have argued that the differences in annotation guidelines for SemEval 2010 Task 8 and the ACE 2005 relation classification task lead to important differences in the resulting learning problems which may make multi-task learning less effective. This suggests a need for a more general consensus on the goal of relation classification as a task if we want to re-use annotated data. We have suggested that reasoning about performance differences between models is difficult because we lack a language for describing what is learnt by convolutional neural networks for natural language processing tasks. Developing such a language would be a helpful tool for pursuing multi-task learning with convolutional neural networks for natural language processing tasks.

Finally we have discussed the need for a unifying theory of multi-task learning that predicts if an auxiliary task should lead to gains in generalization performance. With the current state of the theoretical background, there is no way of knowing if a useless auxiliary task could not be made useful by for example changing the network architecture.

