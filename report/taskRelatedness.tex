\section{Task Relatedness}
Our presentation of multi-task learning so far has been limited to the statistical properties of learning multiple tasks simultaneously without consideration as to how the tasks are related to one another. Intuitively, we expect that learning related tasks should yield better results than learning unrelated tasks.
\\\\
\citet{ben2003} attempts to quantify this intuition by extending the work of \citet{baxter2000} with a notion of task "relatedness". They focus on modeling similarity between the $M$ distributions $P_1$ to $P_M$ from which the $M$ datasets $D_m \in \data_M$ are drawn.

Specifically, they consider two learning tasks, defined by the probability distributions $P_1$ and $P_2$ on the input space $\mathcal{X}$, to be related if $P_1$ and $P_2$ are identical up to a transformation $f: \mathcal{X} \to \mathcal{X}$. To formalize this, they define a set of such transformations $\mathcal{F}$, and say that two learning tasks are $\mathcal{F}$-related if for some fixed distribution the data in each of these tasks are generated by applying some $f \in \mathcal{F}$ to that distribution.
\\\\
Formally, let $\mathcal{F}$ be a set of transformations $f: \mathcal{X} \to \mathcal{X}$, and $P_1$, $P_2$ be probability distributions over $\mathcal{X} \times \mathcal{Y}$ where $\mathcal{Y} = \{0,1\}$. $P_1$ and $P_2$ are $\mathcal{F}$-related if there exists some $f \in \mathcal{F}$ such that for any $T \subseteq \mathcal{X} \times \mathcal{Y}$, $T$ is $P_1$-measurable iff $f[T] = \{(f(\vector{x}), \vector{y}) \mid (\vector{x}, \vector{y}) \in T\}$ is $P_2$-measurable and $P_1(T) = P_2(f[T])$. Two samples are $\mathcal{F}$-related if they are sampled from $\mathcal{F}$-related distributions \citep{ben2003}.
\\\\
In the framework of \citet{ben2003} they assume that the learning system knows the set $\mathcal{F}$ but doesn't know which function $f \in \mathcal{F}$ relates the distributions the system is learning from. Therefore, the ease with which the learner can transfer information about the underlying distributions from one learning task to another depends on the size of $\mathcal{F}$. The larger this set is, the looser the notion of relatedness between the learning tasks.
\\\\
In order to let the learning system take advantage of the multiple datasets $\data_M$, \citet{ben2003} uses their notion of task relatedness to reduce the complexity of the hypothesis space $\mathcal{H}$ by first using all the data $\data_M$ to select a subspace of $\mathcal{H}$ which is likely to contain good solutions to the set of learning problems. After this initial biasing of $\hypspace$, $M$ functions $h_m$ are selected from this subspace for each learning problem. Specifically, from the hypothesis space $\mathcal{H}$, create a family of hypothesis spaces $H$ of sets of hypotheses $h \in \mathcal{H}$ that are equivalent up to transformations in $\mathcal{F}$, assuming that for each $f \in \mathcal{F}$ and $h \in \mathcal{H}$, we have $h \circ f \in \mathcal{H}$. 
\\\\
To formalize this, \citet{ben2003} define an equivalence relation $\sim \mathcal{F}$ on $\mathcal{H}$. This means that $h_1$ and $h_2$ are equivalent if there exists $f\ \in \mathcal{F}$ such that $h_2 = h_1 \circ f$. They use the notation $[h]_{\sim \mathcal{F}}$ to mean the equivalence class of $h$ under $\mathcal{F}$. 

\citet{ben2003} uses the notion of equivalence classes to partition $\mathcal{H}$ into the family $H$ of equivalence classes of $\mathcal{H}$ under $\mathcal{F}$, i.e $H = \mathcal{H} \setminus \sim \mathcal{F}$
\\\\
Note that if two learning tasks defined by the distributions $P_1$ and $P_2$ are $\mathcal{F}$-related, then there exists $f \in \mathcal{F}$ such that the generalization errors of any function $h \in \mathcal{H}$ on both tasks are equal. In other words, there exists $f \in \mathcal{F}$ such that:
$$
E_1(h) = E_2(h \circ f)
$$
This means that the equivalence classes of $\mathcal{H}$ perform equally well on the different tasks, when measured by:
$$
E_m(H) = \inf\limits_{h \in H}E(h)
$$

\citet{ben2003} uses this fact of equivalence classes to build on \citet{baxter2000} and shows that if the number of examples $N$ in each learning task satisfy:
$$
N \geq O\left(\frac{1}{\epsilon^2}\left(d(M, H) \log \frac{1}{\epsilon} + \frac{1}{M} \log \frac{1}{\delta}\right)\right)
$$
Then, with probability $1 - \delta$, for any $1 \leq i \leq M$:
$$
\left| E_i([h]_{\sim \mathcal{F}}) - \inf\limits_{h_1,\dots,h_M \in [h]_{\sim \mathcal{F}}} \frac{1}{M}\sum\limits_{m = 1}^M \hat{E}(h_m, \data_m)\right|  \leq \epsilon
$$
The main difference between this result and the one obtained by \citet{baxter2000} is that \citet{ben2003} bounds the distance between $E_m([h]_{\sim \mathcal{F}})$, i.e the generalization error of the equivalent functions $[h]_{\sim \mathcal{F}}$ for \emph{any} task $m$, and the functions that minimizes the training errors for each $\data_m$, whereas \citet{baxter2000} bounds the distance between the \emph{average} generalization error and training error.
\\\\
This is an important result because it gives credence to our intuition that learning related tasks improves the guarantees that can be made on the distance between training and generalization error over learning unrelated tasks. However, just as the bound provided by \citet{baxter2000}, this bound does not reveal anything about how learning from $\data_M$ might improve $\hat{E}(h_m, \data_m)$. Moreover, the range of domains where tasks are $\mathcal{F}$-related are limited. Specifically, the notion of $\mathcal{F}$-relatedness is limited to domains where two tasks are essentially two different views of the same data, for example video footage of the same scenery from two different perspectives. This may not be an appropriate assumption for natural language processing tasks.
\\\\
The intuition that learning related tasks is more appropriate than learning unrelated tasks is supported by experimental results that indicate that some auxiliary tasks do not lead to gains in target task generalization \citep{bingel2017, luong2015, mou2016, alonso2016, benton2017}. Since investigating which auxiliary tasks are useful by training a multi-task learning system and experimentally comparing generalization dynamics across tasks is impractical, finding target and auxiliary dataset characteristics that indicate gains in target task generalization has received a fair amount of attention.
\\\\
For example, \citet{luong2015} finds that auxiliary datasets which outsize the target dataset lead to worse target task generalization than datasets that are smaller than the target dataset. \citet{bingel2017} finds that auxiliary tasks for which generalization error plateaus more slowly as a function of training examples compared to the target task are the most beneficial. \citet{mou2016} finds that using an auxiliary tasks that's similar to the target task, for example a target and auxiliary relation classification task, leads to better results than learning two different tasks simultaneously, for example an auxiliary named entity recognition task and a target relation classification task. \citet{alonso2016} finds that properties of the label distribution of the auxiliary dataset such as skewness and kurtosis is a good predictor for generalization performance.

However, these results are obtained by investigating statistical correlation between dataset characteristics and target task generalization for a relatively small number of tasks. Since correlation famously does not imply causation, in the absence of an over-arching theory of multi-task learning that fully describes when and how an auxiliary task is beneficial, the only reliable way to determine the usefulness of an auxiliary task is by training a multi-task learning system and testing it empirically.