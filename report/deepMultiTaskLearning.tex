\section{Deep Multi-Task Learning}
\label{deep_multi-task_learning}
Neural networks have the advantage of being easy to adapt from single-task learning to multi-task learning. The simplest way of turning two single task learning problems into a multi-task learning problem using neural networks is by hard weight sharing of a subset of the weights of the networks for the learning tasks and learning them simultaneously \citep{caruana1997}. As an example, consider figure \ref{no_weight_sharing} and \ref{weight_sharing}.
\\\\
\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->, >=stealth, swap]
			\node [neuron] (x11) at (0,0) {};
			\node [neuron] (x12) at (1,0) {};
			\node [neuron] (x13) at (2,0) {};
			\node [neuron] (x14) at (3,0) {};
			\node [neuron] (x15) at (4,0) {};
			\node [neuron] (sigma11) at (1,1.5) {};
			\node [neuron] (sigma12) at (2,1.5) {};
			\node [neuron] (sigma13) at (3,1.5) {};
			\node [neuron] (out1) at (2,3) {};	
			\node [] (label1) at (2,4) {Task 1};	
			\draw (x11) edge (sigma11);
			\draw (x12) edge (sigma11);
			\draw (x13) edge (sigma11);
			
			\draw (x12) edge (sigma12);
			\draw (x13) edge (sigma12);
			\draw (x14) edge (sigma12);
			
			\draw (x13) edge (sigma13);
			\draw (x14) edge (sigma13);
			\draw (x15) edge (sigma13);
			
			\draw (sigma11) edge (out1);
			\draw (sigma12) edge (out1);
			\draw (sigma13) edge (out1);
			
			\node [neuron] (x21) at (6,0) {};
			\node [neuron] (x22) at (7,0) {};
			\node [neuron] (x23) at (8,0) {};
			\node [neuron] (x24) at (9,0) {};
			\node [neuron] (x25) at (10,0) {};
			\node [neuron] (sigma21) at (7,1.5) {};
			\node [neuron] (sigma22) at (8,1.5) {};
			\node [neuron] (sigma23) at (9,1.5) {};
			\node [neuron] (out2) at (8,3) {};	
			\node [] (label2) at (8,4) {Task 2};	
			\draw (x21) edge (sigma21);
			\draw (x22) edge (sigma21);
			\draw (x23) edge (sigma21);
			
			\draw (x22) edge (sigma22);
			\draw (x23) edge (sigma22);
			\draw (x24) edge (sigma22);
			
			\draw (x23) edge (sigma23);
			\draw (x24) edge (sigma23);
			\draw (x25) edge (sigma23);
			
			\draw (sigma21) edge (out2);
			\draw (sigma22) edge (out2);
			\draw (sigma23) edge (out2);
	\end{tikzpicture}
	\caption{Visual representation of single-task learning with neural networks. A set of neural network weights are learnt separately for Task 1 and Task 2.}
	\label{no_weight_sharing}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->, >=stealth, swap]
			\node [neuron] (x1) at (0,0) {};
			\node [neuron] (x2) at (1,0) {};
			\node [neuron] (x3) at (2,0) {};
			\node [neuron] (x4) at (3,0) {};
			\node [neuron] (x5) at (4,0) {};
			\node [neuron] (sigma1) at (1,1.5) {};
			\node [neuron] (sigma2) at (2,1.5) {};
			\node [neuron] (sigma3) at (3,1.5) {};
			\node [neuron] (out1) at (1.3,3) {};	
			\node [neuron] (out2) at (2.7,3) {};
			\node [] (label1) at (1.3,4) {Task 1};
			\node [] (label2) at (2.7,4) {Task 2};
			\draw (x1) edge (sigma1);
			\draw (x2) edge (sigma1);
			\draw (x3) edge (sigma1);
			
			\draw (x2) edge (sigma2);
			\draw (x3) edge (sigma2);
			\draw (x4) edge (sigma2);
			
			\draw (x3) edge (sigma3);
			\draw (x4) edge (sigma3);
			\draw (x5) edge (sigma3);
			
			\draw (sigma1) edge (out1);
			\draw (sigma2) edge (out1);
			\draw (sigma3) edge (out1);
			
			\draw (sigma1) edge (out2);
			\draw (sigma2) edge (out2);
			\draw (sigma3) edge (out2);
	\end{tikzpicture}
	\caption{Visual representation multi-task learning with neural networks. The weights of the hidden layer is shared between the two tasks.}
	\label{weight_sharing}
\end{figure}
\noindent
Multi-task learning techniques that are based on sharing neural network weights between tasks are collectively known as \textbf{deep multi-task learning} techniques. Deep multi-task learning is closely associated with the idea of representation learning presented in section \ref{representation_learning} and the more general framework of bias learning presented in section \ref{bias_learning}. The network represents a shared representation $f(\vector{x})$ represented by $S$ shared layers, often the first layers, and hypotheses $h_1 = (g_1 \circ f)(\vector{x})$ to $h_M = (g_M \circ f)(\vector{x})$ for each learning task, where $g_m$ is $L - S$ neural network layers specific to each task.
\\\\
The exact circumstances under which deep multi-task learning leads to lower overall generalization error compared to deep single-task learning are not yet theoretically well understood. \citet{caruana1997} lists 3 suggestions for how multi-task learning can reduce generalization error:
\begin{description}
	\item [Statistical Data Amplification] The effective number of training examples available to a deep multi-task learning system is increased due to the examples in the auxiliary data. The extensions to the Vapnik-Chervonenkis bound seen in the preceding sections gives us confidence that this reduces the risk that generalization error is far away from training error.
	\item [Eavesdropping] If a hidden layer feature is useful to both Task 1 and Task 2, but much easier to learn when learning Task 2, sharing the hidden layer between the two tasks is likely to reduce generalization error for Task 1.
	\item [Representation Bias] If Task 1 and Task 2 share a common minimum in weight-space, learning the tasks with weight sharing biases the learning system to choose the shared minimum. This is effectively a form of regularization that forces the learning system to search for a good hypothesis in a hypothesis space that is restricted to hypotheses that are useful for more than one task.
\end{description}
\noindent
\citet{baxter2000} applies his bias learning framework to the case where the hypothesis space family $\mathbb{H}$ is constructed of neural networks where the first two layers are shared between tasks.

Specifically, a feature map $\phi_{\vector{w}}:\mathbb{R}^d\mapsto\mathbb{R}^{d^{(2)}}$ is a two layer neural network parameterized by $\vector{w}$ that maps an input vector $\vector{x} \in \mathbb{R}^d$ to feature vector $\phi_{\vector{w}}(\vector{x})$. Each feature $\phi_{\vector{w},j} \in \phi_{\vector{w}}$ is defined by:
$$
\phi_{\vector{w},j}(\vector{x}) = \sigma\left( w_{0j} + \sum\limits_{i=1}^{d^{(1)}} w_{ij}h_i(\vector{x}) \right)
$$
\noindent
$h_{i}$ is the output of unit $i$ in the first layer, $w_{ij}$ is the weight connecting unit $\phi_{\vector{w},j}$ and $i$ and $w_{0j}$ is the bias weight. For simplicity, \citet{baxter2000} considers only the binary threshold activation function:
$$
\sigma(a) = \begin{cases}
	+1 & \text{when } a \geq 0 \\
	-1 & \text{otherwise}
\end{cases}
$$
The output of each unit in the first layer $h_j$ is computed as
$$
h_j(\vector{x}) = \sigma\left( v_{0j} + \sum\limits_{i=1}^{d} v_{ij}x_{i} \right)
$$

where $v_{ij}$ is the weight connecting the input feature $x_i$ to unit $j$ in the first layer and $v_{0j}$ is a bias weight. The total number of weights $W$ in these two layers is thus $W = d^{(1)}(d^{(0)} + 1) + d^{(0)}(d + 1)$. The space of all such feature maps $\{\phi_{\vector{w}} \mid \vector{w} \in \mathbb{R}^W\}$ can be thought of as the representation space $\mathcal{F}$ in the representation learning framework of \citet{baxter1995}.
\\\\
\cite{baxter2000} defines a hypothesis space $\hypspace_{\vector{w}}$ as a set of binary decision functions on top the feature maps. Specifically:
$$
\hypspace_{\vector{w}} = \left\{ \sigma\left(  a_{0} + \sum\limits_{i=1}^{d^{(2)}} a_i\phi_{\vector{w},i}\right) \;\middle| \; a_{0},\dots,a_{d^{(2)}} \in \mathbb{R} \right\}
$$
where $a_i$ is the weight connecting feature $\phi_{\vector{w},i}$ and the output unit and $a_0$ is a bias weight. The set of all such hypothesis spaces can be considered a hypothesis space $\mathbb{H}$ family such that:
$$
\mathbb{H} = \{\hypspace_{\vector{w}} \mid \vector{w} \in \mathbb{R}^W\}
$$
Recall that the goal in bias learning is to select a vector $\vector{h}$ of M hypotheses $h_m$ that minimizes the average generalization error $E(\vector{h}) = \frac{1}{M}\sum_{m=1}^M E_m(h_m)$. With the limitations described above, \citet{baxter2000} is able to show that in order for the average empirical error $\hat{E}(\vector{h}, \data_M)$ to be within $\epsilon$ of the average generalization error $E(\vector{h})$ with probability $1 - \delta$, it suffices that the number of examples $N$ per task satisfies:
$$
N \geq O\left( \frac{1}{\epsilon^2}\left( \frac{W}{M} + d^{(2)} + 1\right)\log \frac{1}{\epsilon} + \frac{1}{M}\log\frac{1}{\delta} \right)
$$
Ignoring the confidence parameters $\epsilon$ and $\delta$, the sample complexity bound tells us that learning complicated neural network representations where $d^{(1)}$ and $d^{(2)}$ and therefore also $W$ are large, is harder than learning simple representations in the sense that it requires more samples to succeed. The benefit gained by multi-task learning is that we can reduce $N$ by increasing $M$, an option we don't have in the single task learning setting. This means we can afford to learn more complicated representations in the hope that this can lead to lower training error for one or more tasks and still have high confidence that generalization is possible by increasing $M$.