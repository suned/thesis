\chapter{Conclusion}
This thesis represents a first step towards understanding whether multi-task learning is a useful technique for improving generalization performance of a deep learning model for relation classification. We began our investigation by exploring relation classification as a natural language processing task. We found that relation classification is a difficult problem because of the high degree of variance and ambiguity of natural language. Because of these challenges, hand-crafted rules that depend on syntactic parses of the input text do not scale well to large inventories of target relations and large scale corpora.
\\\\
We then proceeded to explain how supervised machine learning techniques can be used to solve the relation classification problem as an alternative. We saw how Vapnik-Chervonenkis analysis tells us that the number of training examples $N$ and the complexity of the hypothesis space $m(N, \mathcal{H})$ are the two main conditions governing the success of supervised machine learning techniques.

In continuation, we have introduced convolutional neural networks as a concrete way to implement a hypothesis space that is well suited for multi-task learning. We have discussed how to search this hypothesis space using iterative gradient based methods, and the challenges that comes with this approach.
\\\\
We have explored extensions of Vapnik-Chervonenkis analysis to the multi-task learning setting. These theoretical results in general tells us that learning multiple tasks simultaneously can strengthen the statistical guarantees on the distance between training error $\hat{E}$ and generalization error $E$. However, these theoretical contributions do not show how multi-task learning can lead to increased generalization performance in an absolute sense, by decreasing the training error and the complexity of the hypothesis space simultaneously.
\\\\
We have designed and carried out an experiment that tests the effectiveness of deep multi-task learning on a benchmark relation classification task. Specifically we have searched the research literature and found a convolutional neural network architecture that achieves good results for the relation classification problem. We found however that this architecture made multi-task learning by hard weight sharing impractical. For this reason we adapted this network architecture by proposing three weight sharing strategies:
\begin{itemize}
	\item Shared word and position embeddings in which only the embedding matrices of the target and auxiliary model is shared.
	\item Shared side channel convolutional filters in which we extend the original architecture to a multi-channel convolutional network that allows sharing convolutional filters over a sentence matrix without appended relation argument position information.
	\item Shared convolutional filters in which we share convolutional filters across the augmented sentence matrix that contains information about the relation argument positions. This approach is only practically feasibly when the auxiliary task is another relation classification task.
\end{itemize}
We have empirically compared the sample complexity dynamics of single-task learning and multi-task learning of the target task. Our results are mixed: The shared word and position embedding architecture does not in general lead to statistically significant improvements in target task generalization. The shared side channel architecture leads to occasional improvements of target task generalization. The weight sharing strategy that shares convolutional filters over the augmented sentence matrix reliably produces improvements in generalization error.
\\\\
We have analyzed these results. We have explained that sharing only embedding matrices leads to a low degree of weight sharing when the learning tasks are defined over different corpora. Moreover, we have suggested that the disparate results when using an auxiliary relation classification task may be caused by inconsistent definitions of what constitutes a valid semantic relation. This suggests a need for a consensus on the goal of relation classification as a task if we want to re-use annotated data. Finally, we have discussed the need for a unifying theory of multi-task learning that can provide statistical tests on datasets that can reveal whether implementing a learning system that uses these sets as auxiliary data is wasted effort.
\\\\
We have proposed a number of experiments that might be carried out as future work. Specifically, we have proposed alternative neural network architectures and auxiliary tasks that may lead to better results than we have presented here. We have also suggested a study that compares the sample complexity dynamics of pipelining vs. multi-task learning in order to determine which approach is best suited to reduce the data annotation burden.
\\\\
Our initial results show that deep multi-task learning of relation classification is a feasible strategy for reducing the data annotation burden, provided the designer of the learning system is able to identify the right auxiliary tasks and the right network architecture. As discussed however, in the absence of a theory of multi-task learning that reveals precisely what that entails, this process mostly consists of trail and error. Therefore, there is much work still to be done to better our understanding of how learning several tasks simultaneously can help machines learn how to classify semantic relations.